name: AttentiveSSM_Large_NoProjCyc
dump_dir: /scratch/ya255/lingua/reported_runs/AttentiveSSM_Large_NoProjCyc
seed: 777
grad_acc_steps: 1
gc_collect_freq: 1000
probe_freq: null
steps: 60000
data:
  root_dir: /scratch/ya255/lingua/setup/data/
  sources:
    dclm_baseline_1.0_shuffled: 100.0
  batch_size: 16
  seq_len: 1024
  n_views: 2
  seed: 42
  add_bos: true
  add_eos: true
  load_async: true
  prefetch_size: 512
  tokenizer:
    name: sp
    path: setup/l2tokenizer/tokenizer.model
optim:
  lr: 0.003
  weight_decay: 0.033
  epsilon: 1.0e-08
  beta1: 0.9
  beta2: 0.95
  clip: 1.0
  scheduler: cosine
  warmup: 5000
  lr_min_ratio: 1.0e-06
  cycle_length: 1.0
  cosine_theta: 1.0
  annealing_step: 1000
  exp_factor: 0.5
model:
  dim: 512
  pseudo_chunk: false
  sep_ssm: true
  keep_wproj: false
  fattn_boundary: breakpoint
  ssm_hiddim: 512
  kvssm_dim: 512
  n_layers: 8
  n_heads: 8
  token_chunk: 8
  kv_pressm: false
  n_kv_heads: null
  head_dim: 64
  keep_sink: false
  chunk_strat: cyclic_pl
  additional_tokens: 64
  norm_eps: 1.0e-05
  state_dim: 512
  n_groups: 1
  residual_ssm: true
  conv_size: 4
  dt_bias: false
  D_has_head_dim: false
  learnable_init_states: false
  ssm_heads: 8
  ssm_chunk_size: 64
  vocab_size: 32000
  ffn_dim_multiplier: null
  multiple_of: 256
  rope_theta: 10000.0
  init_use_depth: false
  init_base_std: null
  init_std_factor: disabled
  max_seqlen: 1024
  init_args:
    dt_max: 0.1
    dt_min: 0.001
    dt_init_floor: 0.0001
    A_init_min: 0.01
    A_init_max: 2.0
  seed: 42
  weight_tying: false
  loss_reduction: mean
  sliding_window: null
distributed:
  dp_shard: 1
  dp_replicate: 1
  tp_size: 1
  selective_activation_checkpointing: false
  compile: true
  fsdp_type: full_shard
  model_dtype: bf16
  float8_recipe: null
  float8_filter: layers\.[0-9]+\.
  matmul_allow_tf32: false
  detect_anomaly: false
  compile_cache_size_limit: 8
  spawn_method: forkserver
env:
  MKL_SERVICE_FORCE_INTEL: GNU
  OMP_NUM_THREADS: '1'
  MKL_NUM_THREADS: '1'
  ENABLE_INTRA_NODE_COMM: '1'
  TORCH_NCCL_AVOID_RECORD_STREAMS: '1'
  NCCL_IB_TIMEOUT: '22'
  NCCL_DEBUG: INFO
  TORCH_NCCL_ASYNC_ERROR_HANDLING: '1'
checkpoint:
  dump:
    every: 10000
    keep: 3
  eval:
    every: 5000
    keep: -1
  path: /scratch/ya255/lingua/reported_runs/AttentiveSSM_Large_NoProjCyc/checkpoints
  init_ckpt_path: null
  continue_training_from_init: false
profiling:
  run: true
  trace_folder: profiling
  mem_warmup: 0
  mem_steps: 4
  profile_warmup: 100
  profile_steps: 4
logging:
  freq: 10
  acc_freq: null
  wandb:
    job_type: null
    dir: null
    project: attamba_arxiv
    entity: null
    tags: null
    group: null
    name: AttentiveSSM_Large_NoProjCyc
    notes: null
    config_exclude_keys: null
    config_include_keys: null
    anonymous: null
    mode: null
    allow_val_change: null
    resume: null
    force: null
    tensorboard: null
    sync_tensorboard: null
    monitor_gym: null
    save_code: null
    id: null
    fork_from: null
    resume_from: null
async_eval_gpus: null
eval:
  generator:
    max_tokens: 1024
    dtype: bf16
  harness:
    tasks:
    - wikitext
